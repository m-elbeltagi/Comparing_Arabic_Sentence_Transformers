{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308e2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import datetime\n",
    "import sys \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "\n",
    "# setting device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device in use: {} \\n'.format(device))\n",
    "\n",
    "# setting some parameters\n",
    "batch_size = 16\n",
    "n_epochs = 3\n",
    "num_labels = 5\n",
    "chunk_size = 300\n",
    "         \n",
    "\n",
    "# using 'engine = python' for the second file because it's large \n",
    "poem_train_df = pd.read_csv(r'./poems.csv', engine='python')\n",
    "\n",
    "# extracting unique values in Category column\n",
    "unique_values = poem_train_df['Category'].unique() \n",
    "\n",
    "# map to numerical labels for each category (using index in unique_values)\n",
    "class_mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "\n",
    "# applying class_mapping to Category column\n",
    "poem_train_df['Category'] = poem_train_df['Category'].map(class_mapping)\n",
    "\n",
    "\n",
    "def chunk_text(input_string, chunk_size):\n",
    "    '''Takes a string and splits it into several strings if its length \n",
    "    exceeds chunk_size words, 512 tokens ~380 words, leaving some sapce to \n",
    "    account for special tokens.'''\n",
    "    \n",
    "    words = input_string.split()\n",
    "    chunk_words = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
    "    chunked_strings = [' '.join(chunk) for chunk in chunk_words]\n",
    "    \n",
    "    return chunked_strings\n",
    "\n",
    "\n",
    "# creating a new dataframe with the chunked text\n",
    "chunked_poem_train_df = []\n",
    "chunks = []\n",
    "\n",
    "for _, row in poem_train_df.iterrows():\n",
    "    chunks = chunk_text(row['Poem'], chunk_size)\n",
    "    for chunk in chunks:\n",
    "        chunked_poem_train_df.append({'Poem': chunk, 'Category': row['Category']})\n",
    "        \n",
    "chunked_poem_train_df = pd.DataFrame(chunked_poem_train_df)\n",
    "    \n",
    "    \n",
    "\n",
    "# shuffling dataset\n",
    "chunked_poem_train_df = chunked_poem_train_df.sample(frac=1, \n",
    "                                     random_state=666).reset_index(drop=True)       \n",
    "\n",
    "# renaming columns to what the HF trainer expects\n",
    "chunked_poem_train_df = chunked_poem_train_df.rename(columns={\n",
    "    'Poem': 'text',\n",
    "    'Category': 'label'\n",
    "})\n",
    "\n",
    "\n",
    "# print (len(poem_train_df)) # OG len is 25000\n",
    "# print (len(chunked_poem_train_df)) # New len is 29639\n",
    "\n",
    "pretrained_model_name = 'intfloat/multilingual-e5-small'\n",
    "\n",
    "# tokenizer used needs to match pretrained model \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# loading pretrained model\n",
    "transformer = (AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, \n",
    "                                                                  num_labels=num_labels, \n",
    "                                                                  ignore_mismatched_sizes=True)).to(device)\n",
    "\n",
    "\n",
    "max_context_length = transformer.config.max_position_embeddings\n",
    "\n",
    "def tokenize(batch):\n",
    "    '''padding fills all strings, to match largest string size in the batch. \n",
    "    truncation removes anything longer than context size, just in case.\n",
    "    '''\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True,\n",
    "                     max_length=max_context_length)          \n",
    "\n",
    "\n",
    "full_set = Dataset.from_pandas(chunked_poem_train_df)\n",
    "tokenized_dataset = full_set.map(tokenize, batched=True, \n",
    "                                     batch_size=batch_size)\n",
    "    \n",
    "\n",
    "'''using HF Dataset method \"select\" to split the data, which was already \n",
    "shuffled above (in documentation this select takes a list, but apparently\n",
    "works with range() too)'''\n",
    "train_set = tokenized_dataset.select(range(int(0.8*len(full_set))))\n",
    "eval_set = tokenized_dataset.select(range(int(0.98*len(full_set)), len(full_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e27cfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time is: 2024-02-29 04:57:13.971829 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\Miniconda3\\envs\\spyder-env\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2220' max='4446' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2220/4446 4:08:47 < 4:09:41, 0.15 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.548000</td>\n",
       "      <td>1.435470</td>\n",
       "      <td>0.403035</td>\n",
       "      <td>0.337595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.416400</td>\n",
       "      <td>1.367882</td>\n",
       "      <td>0.436762</td>\n",
       "      <td>0.379531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>1.350600</td>\n",
       "      <td>1.293950</td>\n",
       "      <td>0.516020</td>\n",
       "      <td>0.502592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>1.308200</td>\n",
       "      <td>1.276287</td>\n",
       "      <td>0.480607</td>\n",
       "      <td>0.434947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.287400</td>\n",
       "      <td>1.224032</td>\n",
       "      <td>0.517707</td>\n",
       "      <td>0.458348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>1.235900</td>\n",
       "      <td>1.181504</td>\n",
       "      <td>0.561551</td>\n",
       "      <td>0.503535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>1.217600</td>\n",
       "      <td>1.163688</td>\n",
       "      <td>0.564924</td>\n",
       "      <td>0.517176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>1.184400</td>\n",
       "      <td>1.141190</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>0.503152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>1.114156</td>\n",
       "      <td>0.608769</td>\n",
       "      <td>0.582191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.144600</td>\n",
       "      <td>1.127424</td>\n",
       "      <td>0.554806</td>\n",
       "      <td>0.515834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1628</td>\n",
       "      <td>1.145400</td>\n",
       "      <td>1.079798</td>\n",
       "      <td>0.588533</td>\n",
       "      <td>0.548919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>1.120100</td>\n",
       "      <td>1.086802</td>\n",
       "      <td>0.615514</td>\n",
       "      <td>0.604689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1924</td>\n",
       "      <td>1.113000</td>\n",
       "      <td>1.071849</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.576186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>1.087100</td>\n",
       "      <td>1.028899</td>\n",
       "      <td>0.607083</td>\n",
       "      <td>0.556123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.076100</td>\n",
       "      <td>1.044072</td>\n",
       "      <td>0.578415</td>\n",
       "      <td>0.549637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finsih time is: 2024-02-29 09:06:08.051283 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "logging_steps = len(train_set) // batch_size // 10\n",
    "training_args  = TrainingArguments(output_dir= r'./checkpoints',\n",
    "                                    num_train_epochs=n_epochs,\n",
    "                                    learning_rate=1e-5,\n",
    "                                    per_device_train_batch_size=batch_size,\n",
    "                                    per_device_eval_batch_size=batch_size,\n",
    "                                    load_best_model_at_end=True,\n",
    "                                    metric_for_best_model='f1',\n",
    "                                    weight_decay=0.01,\n",
    "                                    evaluation_strategy='steps',\n",
    "                                    disable_tqdm=False,\n",
    "                                    logging_steps=logging_steps,\n",
    "                                    eval_steps=logging_steps,\n",
    "                                    save_steps=logging_steps,\n",
    "                                    log_level='error',\n",
    "                                    report_to='tensorboard')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(model=transformer,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_set,\n",
    "                    eval_dataset=eval_set,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "\n",
    "\n",
    "\n",
    "print('start time is: {} \\n'.format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print('Finsih time is: {} \\n'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write GitHub post comparing performance of these models on single GPU, make table comparing\n",
    "# see how many authors in test data actually match train data, to see how muc hthe score would go up\n",
    "# multilingual-e5-small ~59% acc (1.5 epochs)\n",
    "# without chunking the performance was ~67% acc (for 5 epochs), seems chunking plateaus at ~68% acc (for 5 epochs),distilbert-base-multilingual-cased\n",
    "# no chunking, no learning scheduler ~65% acc (early stopped at 2 epochs)\n",
    "# medmediani/Arabic-KW-Mdel ~57% acc (early stopping)\n",
    "# frozen apporach with pytroch ~50% acc\n",
    "# check author first, if not in database, then use model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
