{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import datetime\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "# setting device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device in use: {} \\n'.format(device))\n",
    "\n",
    "# setting some parameters\n",
    "batch_size = 16\n",
    "n_epochs = 3\n",
    "num_labels = 5\n",
    "chunk_size = 300\n",
    "         \n",
    "\n",
    "# using 'engine = python' for the second file because it's large \n",
    "poem_train_df = pd.read_csv(r'./poems.csv', engine='python')\n",
    "\n",
    "# extracting unique values in Category column\n",
    "unique_values = poem_train_df['Category'].unique() \n",
    "\n",
    "# map to numerical labels for each category (using index in unique_values)\n",
    "class_mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "\n",
    "# applying class_mapping to Category column\n",
    "poem_train_df['Category'] = poem_train_df['Category'].map(class_mapping)\n",
    "\n",
    "\n",
    "def chunk_text(input_string, chunk_size):\n",
    "    '''Takes a string and splits it into several strings if its length \n",
    "    exceeds chunk_size words, 512 tokens ~380 words, leaving some sapce to \n",
    "    account for special tokens.'''\n",
    "    \n",
    "    words = input_string.split()\n",
    "    chunk_words = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
    "    chunked_strings = [' '.join(chunk) for chunk in chunk_words]\n",
    "    \n",
    "    return chunked_strings\n",
    "\n",
    "\n",
    "# creating a new dataframe with the chunked text\n",
    "chunked_poem_train_df = []\n",
    "chunks = []\n",
    "\n",
    "for _, row in poem_train_df.iterrows():\n",
    "    chunks = chunk_text(row['Poem'], chunk_size)\n",
    "    for chunk in chunks:\n",
    "        chunked_poem_train_df.append({'Poem': chunk, 'Category': row['Category']})\n",
    "        \n",
    "chunked_poem_train_df = pd.DataFrame(chunked_poem_train_df)\n",
    "    \n",
    "    \n",
    "\n",
    "# shuffling dataset\n",
    "chunked_poem_train_df = chunked_poem_train_df.sample(frac=1, \n",
    "                                     random_state=666).reset_index(drop=True)       \n",
    "\n",
    "# renaming columns to what the HF trainer expects\n",
    "chunked_poem_train_df = chunked_poem_train_df.rename(columns={\n",
    "    'Poem': 'text',\n",
    "    'Category': 'label'\n",
    "})\n",
    "\n",
    "\n",
    "# print (len(poem_train_df)) # OG len is 25000\n",
    "# print (len(chunked_poem_train_df)) # New len is 29639\n",
    "\n",
    "pretrained_model_name = 'intfloat/multilingual-e5-small'\n",
    "\n",
    "# tokenizer used needs to match pretrained model \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# loading pretrained model\n",
    "transformer = (AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, \n",
    "                                                                  num_labels=num_labels, \n",
    "                                                                  ignore_mismatched_sizes=True)).to(device)\n",
    "\n",
    "\n",
    "max_context_length = transformer.config.max_position_embeddings\n",
    "\n",
    "def tokenize(batch):\n",
    "    '''padding fills all strings, to match largest string size in the batch. \n",
    "    truncation removes anything longer than context size, just in case.\n",
    "    '''\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True,\n",
    "                     max_length=max_context_length)          \n",
    "\n",
    "\n",
    "full_set = Dataset.from_pandas(chunked_poem_train_df)\n",
    "tokenized_dataset = full_set.map(tokenize, batched=True, \n",
    "                                     batch_size=batch_size)\n",
    "    \n",
    "\n",
    "'''using HF Dataset method \"select\" to split the data, which was already \n",
    "shuffled above (in documentation this select takes a list, but apparently\n",
    "works with range() too)'''\n",
    "train_set = tokenized_dataset.select(range(int(0.8*len(full_set))))\n",
    "eval_set = tokenized_dataset.select(range(int(0.8*len(full_set)), len(full_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "logging_steps = len(train_set) // batch_size // 10\n",
    "training_args  = TrainingArguments(output_dir= r'./checkpoints',\n",
    "                                    num_train_epochs=n_epochs,\n",
    "                                    learning_rate=1e-5,\n",
    "                                    warmup_steps=1000,\n",
    "                                    lr_scheduler_type='linear',\n",
    "                                    per_device_train_batch_size=batch_size,\n",
    "                                    per_device_eval_batch_size=batch_size,\n",
    "                                    load_best_model_at_end=True,\n",
    "                                    metric_for_best_model='f1',\n",
    "                                    weight_decay=0.01,\n",
    "                                    evaluation_strategy='steps',\n",
    "                                    disable_tqdm=False,\n",
    "                                    logging_steps=logging_steps,\n",
    "                                    eval_steps=logging_steps,\n",
    "                                    save_steps=logging_steps,\n",
    "                                    log_level='error',\n",
    "                                    report_to='tensorboard')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(model=transformer,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_set,\n",
    "                    eval_dataset=eval_set,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "\n",
    "\n",
    "\n",
    "print('start time is: {} \\n'.format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print('Finsih time is: {} \\n'.format(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
